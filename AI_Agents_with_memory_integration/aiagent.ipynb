{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dKrGZ6-KsbgM"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List\n",
        "import openai\n",
        "\n",
        "class OpenAIChatCompletion:\n",
        "    \"\"\"\n",
        "    Interacts with OpenAI's API for chat completions.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
        "        \"\"\"\n",
        "        Initialize with model, API key, and base URL.\n",
        "        \"\"\"\n",
        "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
        "        self.model = model\n",
        "\n",
        "    def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a response from input messages.\n",
        "        \"\"\"\n",
        "        params = {'messages': messages, 'model': self.model, **kwargs}\n",
        "        response = self.client.chat.completions.create(**params)\n",
        "        return response.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Integrates LLM client, tools, and memory.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm_client, tools=None, memory=None):\n",
        "        self.llm_client = llm_client\n",
        "        self.tools = tools\n",
        "        self.memory = memory\n",
        "\n",
        "    def run(self, messages: List[Dict[str, str]]):\n",
        "        \"\"\"\n",
        "        Generates a response from the LLM client.\n",
        "        \"\"\"\n",
        "        # Generate response using the LLM client\n",
        "        response = self.llm_client.generate(messages)\n",
        "        return response"
      ],
      "metadata": {
        "id": "NOsqyU40s9pP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Ask user for API key securely in Colab\n",
        "OPENAI_API_KEY = getpass.getpass(\"Enter your API Key: \")\n",
        "\n",
        "# Initialize the client with a custom base_url (OpenRouter in this case)\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# --- Define the Agent class ---\n",
        "class Agent:\n",
        "    def __init__(self, llm_client, model=\"openai/gpt-oss-20b\"):\n",
        "        self.llm_client = llm_client\n",
        "        self.model = model\n",
        "\n",
        "    def run(self, messages):\n",
        "        \"\"\"\n",
        "        Run the agent with given messages and return the response.\n",
        "        \"\"\"\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# --- Example usage ---\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a security assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hey! This is Roberto!\"}\n",
        "]\n",
        "\n",
        "myAgent = Agent(llm_client=client)\n",
        "\n",
        "response = myAgent.run(messages=messages)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWLnuN58uSJU",
        "outputId": "134a71c3-530b-493e-e342-1164f9bc639b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Hi Roberto! ðŸ‘‹ How can I help you with security today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "class ChatMessageMemory:\n",
        "    \"\"\"Manages conversation context.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.messages = []\n",
        "\n",
        "    def add_message(self, message: Dict):\n",
        "        \"\"\"Add a message to memory.\"\"\"\n",
        "        self.messages.append(message)\n",
        "\n",
        "    def add_messages(self, messages: List[Dict]):\n",
        "        \"\"\"Add multiple messages to memory.\"\"\"\n",
        "        for message in messages:\n",
        "            self.add_message(message)\n",
        "\n",
        "    def add_conversation(self, user_message: Dict, assistant_message: Dict):\n",
        "        \"\"\"Add a user-assistant conversation.\"\"\"\n",
        "        self.add_messages([user_message, assistant_message])\n",
        "\n",
        "    def get_messages(self) -> List[Dict]:\n",
        "        \"\"\"Retrieve all messages.\"\"\"\n",
        "        return self.messages.copy()\n",
        "\n",
        "    def reset_memory(self):\n",
        "        \"\"\"Clear all messages.\"\"\"\n",
        "        self.messages = []"
      ],
      "metadata": {
        "id": "bbtsBuwv-x0Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    \"\"\"Integrates LLM client, tools, and memory.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_client, system_message: Dict[str, str], tools=None):\n",
        "        self.llm_client = llm_client\n",
        "        self.tools = tools\n",
        "        self.memory = ChatMessageMemory()\n",
        "        self.system_message = system_message\n",
        "\n",
        "    def run(self, user_message: Dict[str, str]):\n",
        "        \"\"\"Generate a response using LLM client and store context.\"\"\"\n",
        "        self.memory.add_message(user_message)\n",
        "        chat_history = [self.system_message] + self.memory.get_messages()\n",
        "        response = self.llm_client.generate(chat_history)\n",
        "        self.memory.add_message(response)\n",
        "        return response"
      ],
      "metadata": {
        "id": "sLn2RDoJ_Zpv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Ask for OpenRouter API key securely\n",
        "OPENROUTER_API_KEY = getpass.getpass(\"Enter your OpenRouter API Key: \")\n",
        "\n",
        "# Initialize the client with base_url + api_key\n",
        "client = OpenAI(\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# --- Define the Agent class with memory ---\n",
        "class Agent:\n",
        "    def __init__(self, llm_client, system_message, model=\"openai/gpt-oss-20b\"):\n",
        "        self.llm_client = llm_client\n",
        "        self.system_message = system_message\n",
        "        self.model = model\n",
        "        self.chat_history = [system_message]  # keep memory\n",
        "\n",
        "    def run(self, user_message):\n",
        "        # Add user message to history\n",
        "        self.chat_history.append(user_message)\n",
        "\n",
        "        # Call the model\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=self.chat_history\n",
        "        )\n",
        "\n",
        "        # Extract the assistant reply\n",
        "        reply = response.choices[0].message.content\n",
        "\n",
        "        # Add assistant reply to history for memory\n",
        "        self.chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "        return reply\n",
        "\n",
        "\n",
        "# --- Example usage ---\n",
        "system_message = {\"role\": \"system\", \"content\": \"You are a security assistant.\"}\n",
        "agent = Agent(llm_client=client, system_message=system_message)\n",
        "\n",
        "# First user message\n",
        "user_message = {\"role\": \"user\", \"content\": \"Hey! This is Roberto!\"}\n",
        "print(agent.run(user_message))\n",
        "\n",
        "# Follow-up message (tests memory)\n",
        "follow_up_message = {\"role\": \"user\", \"content\": \"What was my name?\"}\n",
        "print(agent.run(follow_up_message))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CivwTWiN_gcW",
        "outputId": "bcf37a43-3fe7-44df-fda9-820f9278a972"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Hey Roberto! How can I help you with security today?\n",
            "Your name was Roberto.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any, List\n",
        "import openai\n",
        "\n",
        "class OpenAIChatCompletion:\n",
        "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
        "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
        "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
        "        self.model = model\n",
        "\n",
        "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
        "        params = {'messages': messages, 'model': self.model, 'tools': tools, **kwargs}\n",
        "        response = self.client.chat.completions.create(**params)\n",
        "        return response.choices[0].message"
      ],
      "metadata": {
        "id": "YJefpRY3ByaG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"Integrates LLM client, tools, and memory.\"\"\"\n",
        "    def __init__(self, llm_client, system_message: Dict[str, str], tools=None):\n",
        "        self.llm_client = llm_client\n",
        "        self.tools = tools\n",
        "        self.memory = ChatMessageMemory()\n",
        "        self.system_message = system_message\n",
        "\n",
        "    def run(self, user_message: Dict[str, str]):\n",
        "        self.memory.add_message(user_message)\n",
        "        chat_history = [self.system_message] + self.memory.get_messages()\n",
        "        response = self.llm_client.generate(chat_history, tools=self.tools)\n",
        "        self.memory.add_message(response)\n",
        "        return response"
      ],
      "metadata": {
        "id": "ooAdqQLeB4Oh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Gets weather information.\"\"\"\n",
        "    return f\"{location}: 80F.\"\n",
        "\n",
        "def jump(distance: str) -> str:\n",
        "    \"\"\"Jumps a specific distance.\"\"\"\n",
        "    return f\"I jumped the following distance {distance}\""
      ],
      "metadata": {
        "id": "6lp0oYWcB-Mb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather_func_dict = {\n",
        "    'type': 'function',\n",
        "    'function': {\n",
        "        'name': 'get_weather',\n",
        "        'description': 'Get weather information based on location.',\n",
        "        'parameters': {\n",
        "            'properties': {'location': {'type': 'string'}},\n",
        "            'required': ['location'],\n",
        "            'type': 'object'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "jump_func_dict = {\n",
        "    'type': 'function',\n",
        "    'function': {\n",
        "        'name': 'jump',\n",
        "        'description': 'Jump a specific distance.',\n",
        "        'parameters': {\n",
        "            'properties': {'distance': {'type': 'string'}},\n",
        "            'required': ['distance'],\n",
        "            'type': 'object'\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "TZEev8HvCCRY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# ðŸ”‘ Enter your API key securely in Colab\n",
        "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# âœ… Initialize client\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# --- Define the Agent class with tools ---\n",
        "class Agent:\n",
        "    def __init__(self, llm_client, system_message, tools=None, model=\"openai/gpt-oss-20b\"):\n",
        "        self.llm_client = llm_client\n",
        "        self.system_message = system_message\n",
        "        self.model = model\n",
        "        self.chat_history = [system_message]\n",
        "        self.tools = tools if tools else []\n",
        "\n",
        "    def run(self, user_message):\n",
        "        # Add user message to history\n",
        "        self.chat_history.append(user_message)\n",
        "\n",
        "        # Send to LLM with optional tools\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=self.chat_history,\n",
        "            tools=self.tools if self.tools else None\n",
        "        )\n",
        "\n",
        "        # Extract assistant reply\n",
        "        reply = response.choices[0].message\n",
        "\n",
        "        # Store reply in chat history\n",
        "        self.chat_history.append({\"role\": reply.role, \"content\": reply.content})\n",
        "\n",
        "        return reply\n",
        "\n",
        "\n",
        "# --- Example tools (replace with your own dicts) ---\n",
        "get_weather_func_dict = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get the weather for a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "jump_func_dict = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"jump\",\n",
        "        \"description\": \"Make the assistant jump\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {}\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Usage ---\n",
        "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "tools = [get_weather_func_dict, jump_func_dict]\n",
        "\n",
        "agent = Agent(llm_client=client, system_message=system_message, tools=tools, model=\"openai/gpt-oss-20b\")\n",
        "\n",
        "# First user query\n",
        "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia?\"}\n",
        "response = agent.run(user_message)\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0OZmO9xCGMs",
        "outputId": "cd09ce85-a7d8-40a4-bf3b-218979fd2284"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_S72OEAtsvUmrScufyzINy8oE', function=Function(arguments='{\"location\":\"Virginia\"}', name='get_weather'), type='function', index=0)], reasoning='User wants weather in Virginia. Need to call get_weather with location \"Virginia\".', reasoning_details=[{'type': 'reasoning.text', 'text': 'User wants weather in Virginia. Need to call get_weather with location \"Virginia\".', 'format': 'unknown', 'index': 0}])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "tool_response = response.tool_calls[0]\n",
        "tool_arguments = json.loads(tool_response.function.arguments)\n",
        "tool_arguments\n",
        "\n",
        "tool_execution_results = get_weather(**tool_arguments)\n",
        "tool_execution_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sUgFHxGVGe6L",
        "outputId": "95be944f-2459-4093-fe43-da993f6f4920"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Virginia: 80F.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_message = {\n",
        "    \"role\": \"tool\",\n",
        "    \"tool_call_id\": tool_response.id,\n",
        "    \"name\": tool_response.function.name,\n",
        "    \"content\": str(tool_execution_results)\n",
        "}\n",
        "\n",
        "final_response = agent.run(tool_message)\n",
        "final_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJm4fmyKG1AJ",
        "outputId": "98a24e88-969c-4ea3-d379-88d8804b68b7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content='Itâ€™s currently warm in Virginiaâ€”around 80\\u202fÂ°F.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='The user asked: \"What is the weather in Virginia?\" The system performed a function call and got a response: Virginia: 80F. This is presumably the correct answer. But the user now expects an answer. However, the tool call returned \"Virginia: 80F.\" We need to output a final answer. However, currently we have a non-true expected result? The function call \"get_weather\" returns \"Virginia: 80F.\" The assistant should give that answer, potentially with an explanation. The user wants the weather. So answer: \"It is about 80Â°F\". Should we elaborate? Unsure. I think a simple response: \"The weather in Virginia is currently around 80Â°F.\" That would satisfy.', reasoning_details=[{'type': 'reasoning.text', 'text': 'The user asked: \"What is the weather in Virginia?\" The system performed a function call and got a response: Virginia: 80F. This is presumably the correct answer. But the user now expects an answer. However, the tool call returned \"Virginia: 80F.\" We need to output a final answer. However, currently we have a non-true expected result? The function call \"get_weather\" returns \"Virginia: 80F.\" The assistant should give that answer, potentially with an explanation. The user wants the weather. So answer: \"It is about 80Â°F\". Should we elaborate? Unsure. I think a simple response: \"The weather in Virginia is currently around 80Â°F.\" That would satisfy.', 'format': 'unknown', 'index': 0}])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python function\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get weather information based on location.\"\"\"\n",
        "    return f\"{location}: 80F.\"\n",
        "\n",
        "# Pydantic Model\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GetWeatherSchema(BaseModel):\n",
        "    \"\"\"Get weather information based on location.\"\"\"\n",
        "    location: str = Field(description=\"Location to get weather for\")"
      ],
      "metadata": {
        "id": "sxdwZjhOG3jD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract tool arguments (JSON string)\n",
        "tool_response = response.tool_calls[0]\n",
        "tool_arguments = tool_response.function.arguments\n",
        "\n",
        "response_model = GetWeatherSchema.model_validate_json(tool_arguments)\n",
        "response_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRfxZqhCHBcE",
        "outputId": "79b5f05b-7473-4c43-a018-7fe4669c02da"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GetWeatherSchema(location='Virginia')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GetWeatherSchema.model_json_schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p91cx4zEHLnM",
        "outputId": "e75f7fda-1cfd-4619-bbcb-fca1df4773c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': 'Get weather information based on location.',\n",
              " 'properties': {'location': {'description': 'Location to get weather for',\n",
              "   'title': 'Location',\n",
              "   'type': 'string'}},\n",
              " 'required': ['location'],\n",
              " 'title': 'GetWeatherSchema',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools: [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"function_name\",\n",
        "            \"description\": \"Function description. (optional)\",\n",
        "            \"parameters\": \"The parameters the functions accepts, described as a JSON Schema object. (optional)\"\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Ex9PjBmiHQXY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_openai_function_call_definition(name: str, model: BaseModel):\n",
        "    schema_dict = model.model_json_schema()\n",
        "    description = schema_dict.pop(\"description\", \"\")\n",
        "    schema_dict.pop(\"title\", None)  # Remove the title field to exclude the model name\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": name,\n",
        "            \"description\": description,\n",
        "            \"parameters\": schema_dict\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "ltMBXxIGIH6Z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_call_definition = to_openai_function_call_definition(\"get_weather\", GetWeatherSchema)\n",
        "function_call_definition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldLyy64HIJfS",
        "outputId": "b4e7768c-1147-424e-e271-ec2ec8f18823"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'function',\n",
              " 'function': {'name': 'get_weather',\n",
              "  'description': 'Get weather information based on location.',\n",
              "  'parameters': {'properties': {'location': {'description': 'Location to get weather for',\n",
              "     'title': 'Location',\n",
              "     'type': 'string'}},\n",
              "   'required': ['location'],\n",
              "   'type': 'object'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import Callable, Type\n",
        "\n",
        "class AgentTool:\n",
        "    \"\"\"Encapsulates a Python function with Pydantic validation.\"\"\"\n",
        "    def __init__(self, func: Callable, args_model: Type[BaseModel]):\n",
        "        self.func = func\n",
        "        self.args_model = args_model\n",
        "        self.name = func.__name__\n",
        "        self.description = func.__doc__ or self.args_schema.get('description', '')\n",
        "\n",
        "    def to_openai_function_call_definition(self) -> dict:\n",
        "        \"\"\"Converts the tool to OpenAI Function Calling format.\"\"\"\n",
        "        schema_dict = self.args_schema\n",
        "        description = schema_dict.pop(\"description\", \"\")\n",
        "        return {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": self.name,\n",
        "                \"description\": description,\n",
        "                \"parameters\": schema_dict\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def args_schema(self) -> dict:\n",
        "        \"\"\"Returns the tool's function argument schema as a dictionary.\"\"\"\n",
        "        schema = self.args_model.model_json_schema()\n",
        "        schema.pop(\"title\", None)\n",
        "        return schema"
      ],
      "metadata": {
        "id": "Pp2xKR6lIWjA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Optional, Type\n",
        "from pydantic import BaseModel\n",
        "\n",
        "def check_docstring(func: Callable):\n",
        "    \"\"\"Ensure the function has a docstring.\"\"\"\n",
        "    if not func.__doc__:\n",
        "        raise ValueError(f\"Function '{func.__name__}' must have a docstring.\")\n",
        "\n",
        "def Tool(func: Optional[Callable] = None, *, args_model: Type[BaseModel]) -> AgentTool:\n",
        "    \"\"\"Decorator to wrap a function with an AgentTool instance.\"\"\"\n",
        "    def decorator(f: Callable) -> AgentTool:\n",
        "        check_docstring(f)\n",
        "        return AgentTool(f, args_model=args_model)\n",
        "    return decorator(func) if func else decorator"
      ],
      "metadata": {
        "id": "diNZEaKmIYWV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python function arguments schema\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GetWeatherSchema(BaseModel):\n",
        "    location: str = Field(description=\"location to get weather for\")\n",
        "\n",
        "@Tool(args_model=GetWeatherSchema)\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get weather information based on location.\"\"\"\n",
        "    return f\"{location}: 80F.\"\n",
        "\n",
        "# Convert the AgentTool to OpenAI's Function Calling format\n",
        "get_weather.to_openai_function_call_definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV_874MaItz0",
        "outputId": "bf721fe9-1a23-4484-cd14-dc5ba663b901"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'function',\n",
              " 'function': {'name': 'get_weather',\n",
              "  'description': '',\n",
              "  'parameters': {'properties': {'location': {'description': 'location to get weather for',\n",
              "     'title': 'Location',\n",
              "     'type': 'string'}},\n",
              "   'required': ['location'],\n",
              "   'type': 'object'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, ValidationError\n",
        "from typing import Callable, Type\n",
        "from inspect import signature\n",
        "\n",
        "class AgentTool:\n",
        "    # Existing code...\n",
        "\n",
        "    def validate_json_args(self, json_string: str) -> bool:\n",
        "        \"\"\"Validate JSON string using the Pydantic model.\"\"\"\n",
        "        try:\n",
        "            validated_args = self.args_model.model_validate_json(json_string)\n",
        "            return isinstance(validated_args, self.args_model)\n",
        "        except ValidationError:\n",
        "            return False\n",
        "\n",
        "    def run(self, *args, **kwargs) -> Any:\n",
        "        \"\"\"Execute the function with validated arguments.\"\"\"\n",
        "        try:\n",
        "            # Handle positional arguments by converting them to keyword arguments\n",
        "            if args:\n",
        "                sig = signature(self.func)\n",
        "                arg_names = list(sig.parameters.keys())\n",
        "                kwargs.update(dict(zip(arg_names, args)))\n",
        "\n",
        "            # Validate arguments with the provided Pydantic schema\n",
        "            validated_args = self.args_model(**kwargs)\n",
        "            return self.func(**validated_args.model_dump())\n",
        "        except ValidationError as e:\n",
        "            raise ValueError(f\"Argument validation failed for tool '{self.name}': {str(e)}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"An error occurred during the execution of tool '{self.name}': {str(e)}\")\n",
        "\n",
        "    def __call__(self, *args, **kwargs) -> Any:\n",
        "        \"\"\"Allow the AgentTool instance to be called like a regular function.\"\"\"\n",
        "        return self.run(*args, **kwargs)"
      ],
      "metadata": {
        "id": "mGBysIihI44d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# --- Define the actual tool function ---\n",
        "def get_weather(location: str):\n",
        "    \"\"\"Mock weather function\"\"\"\n",
        "    return f\"The weather in {location} is sunny and 25Â°C.\"\n",
        "\n",
        "# --- Validate JSON string arguments ---\n",
        "tool_json_arguments = '{ \"location\": \"Virginia\" }'\n",
        "\n",
        "try:\n",
        "    # Convert JSON string to dict\n",
        "    tool_arguments_dict = json.loads(tool_json_arguments)\n",
        "\n",
        "    # Run the tool with unpacked arguments\n",
        "    result = get_weather(**tool_arguments_dict)\n",
        "\n",
        "    print(\"âœ… Tool executed successfully:\", result)\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"âŒ Invalid JSON:\", e)\n",
        "except TypeError as e:\n",
        "    print(\"âŒ Argument mismatch:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bex5ZRFOI_9U",
        "outputId": "4250d8f9-9b5f-47f8-8f92-dfdb554b4380"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Tool executed successfully: The weather in Virginia is sunny and 25Â°C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "class AgentToolExecutor:\n",
        "    \"\"\"Manages tool registration and execution.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: Optional[List[AgentTool]] = None):\n",
        "        self.tools: Dict[str, AgentTool] = {}\n",
        "        if tools:\n",
        "            for tool in tools:\n",
        "                self.register_tool(tool)\n",
        "\n",
        "    def register_tool(self, tool: AgentTool):\n",
        "        \"\"\"Registers a tool.\"\"\"\n",
        "        if tool.name in self.tools:\n",
        "            raise ValueError(f\"Tool '{tool.name}' is already registered.\")\n",
        "        self.tools[tool.name] = tool\n",
        "\n",
        "    def execute(self, tool_name: str, *args, **kwargs) -> Any:\n",
        "        \"\"\"Executes a tool by name with given arguments.\"\"\"\n",
        "        tool = self.tools.get(tool_name)\n",
        "        if not tool:\n",
        "            raise ValueError(f\"Tool '{tool_name}' not found.\")\n",
        "        try:\n",
        "            return tool(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error executing tool '{tool_name}': {e}\") from e\n",
        "\n",
        "    def get_tool_names(self) -> List[str]:\n",
        "        \"\"\"Returns a list of all registered tool names.\"\"\"\n",
        "        return list(self.tools.keys())\n",
        "\n",
        "    def get_tool_details(self) -> str:\n",
        "        \"\"\"Returns details of all registered tools.\"\"\"\n",
        "        tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in self.tools.values()]\n",
        "        return '\\n'.join(tools_info)"
      ],
      "metadata": {
        "id": "5j17pP4IK5ir"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"Integrates LLM client, tools, memory, and manages tool executions.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None):\n",
        "        self.llm_client = llm_client\n",
        "        self.executor = AgentToolExecutor()\n",
        "        self.memory = ChatMessageMemory()\n",
        "        self.system_message = system_message\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tool_history = []\n",
        "        self.function_calls = None\n",
        "\n",
        "        # Register and convert tools\n",
        "        if tools:\n",
        "            for tool in tools:\n",
        "                self.executor.register_tool(tool)\n",
        "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
        "\n",
        "    def run(self, user_message: Dict[str, str]):\n",
        "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
        "        self.memory.add_message(user_message)\n",
        "\n",
        "        for _ in range(self.max_iterations):\n",
        "            chat_history = [self.system_message] + self.memory.get_messages() + self.tool_history\n",
        "            response = self.llm_client.generate(chat_history, tools=self.function_calls)\n",
        "\n",
        "            if self.parse_response(response):\n",
        "                continue\n",
        "            else:\n",
        "                self.memory.add_message(response)\n",
        "                self.tool_history = []\n",
        "                return response\n",
        "\n",
        "    def parse_response(self, response) -> bool:\n",
        "        \"\"\"Executes tool calls suggested by the LLM and updates tool history.\"\"\"\n",
        "        import json\n",
        "\n",
        "        if response.tool_calls:\n",
        "            self.tool_history.append(response)\n",
        "            for tool in response.tool_calls:\n",
        "                tool_name = tool.function.name\n",
        "                tool_args = tool.function.arguments\n",
        "                tool_args_dict = json.loads(tool_args)\n",
        "                try:\n",
        "                    logger.info(f\"Executing {tool_name} with args: {tool_args}\")\n",
        "                    execution_results = self.executor.execute(tool_name, **tool_args_dict)\n",
        "                    self.tool_history.append({\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_call_id\": tool.id,\n",
        "                        \"name\": tool_name,\n",
        "                        \"content\": str(execution_results)\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    raise ValueError(f\"Execution error in tool '{tool_name}': {e}\") from e\n",
        "            return True\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "c8N0w7kMLC2E"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system message\n",
        "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "\n",
        "# Initialize the Agent with the LLM client, system message, and tools\n",
        "agent = Agent(llm_client=client, system_message=system_message, tools=tools)\n",
        "\n",
        "# Run Task\n",
        "# Define a user message\n",
        "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia?\"}\n",
        "\n",
        "# Generate a response using the agent\n",
        "response = agent.run(user_message)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gwjg_aFNUts",
        "outputId": "253dc60c-08b9-443b-bf3a-acbee7b9778d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The weather in Virginia is sunny and 25Â°C.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable Logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Reset previous interactions\n",
        "agent.memory.reset_memory()\n",
        "\n",
        "# Define a new task\n",
        "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia, Washington and New York?\"}\n",
        "\n",
        "# Generate a response using the agent\n",
        "response = agent.run(user_message)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "c4_5bOmVojt5",
        "outputId": "83757a87-e44f-46e5-c23c-779a9fa5fb80"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Agent' object has no attribute 'memory'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3771302116.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Reset previous interactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Define a new task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'memory'"
          ]
        }
      ]
    }
  ]
}