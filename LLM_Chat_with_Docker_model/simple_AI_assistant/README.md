# ğŸ§  Simple AI Assistant Docker App that Remembers

This project is a **production-ready AI chat app** built with **Docker Model Runner**, **Streamlit**, and **LangChain**.
It lets you talk to a **local LLM running via Docker**, or switch seamlessly to a **large cloud-based model (like OpenRouter)** â€” all while remembering your entire conversation history.
Each model gets the full chat context, even when it wasnâ€™t active earlier.

---

## â­ Features

âœ… Run local open-source LLMs with Docker Model Runner ğŸ¤–
âœ… Clean Streamlit chat interface with message history ğŸ’»
âœ… Seamless switch between local and cloud models ğŸ•¹ï¸
âœ… Context-passing for memory-aware responses ğŸ’¡
âœ… Fully containerized with Docker Compose ğŸ‹

---

## ğŸ“¸ Screenshot

<img width="1368" height="698" alt="image" src="https://github.com/user-attachments/assets/24f17fe6-3c46-4717-a368-1b0330a547c9" />


---

## âš¡ï¸ Quick Start

### 1ï¸âƒ£ Prerequisites

* Install the latest **Docker Desktop (v4.45+)**
* Enable **WSL 2** integration (Windows only)
* Ensure **Docker Model Runner** is active:

```bash
docker model status
```

âœ… Output should say: `Docker Model Runner is running`

---

### 2ï¸âƒ£ Enable & Test Models

Pull a model (e.g., **Gemma3**) from Dockerâ€™s Model Catalog:

```bash
# Pull model
docker model pull ai/gemma3

# Verify
docker model list

# Quick test with curl
curl http://model-runner.docker.internal/engines/llama.cpp/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "ai/gemma3",
        "messages": [{"role": "user", "content": "Hello, explain Docker in one line."}]
      }'
```

---

### 3ï¸âƒ£ Clone This Repo

```bash
git clone https://github.com/MariyaSha/simple_AI_assistant.git
cd simple_AI_assistant
```

---

### 4ï¸âƒ£ Create a `.env` File

Create a file named `.env` in the project root:

```env
LOCAL_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
REMOTE_BASE_URL=https://openrouter.ai/api/v1
LOCAL_MODEL_NAME=ai/gemma3
REMOTE_MODEL_NAME=qwen/qwen3-30b-a3b
OPENROUTER_API_KEY=YOUR_OPENROUTER_API_KEY
```

* Replace `YOUR_OPENROUTER_API_KEY` with your actual OpenRouter key.
* Pick any model from ğŸ‘‰ [Docker Model Catalog](https://catalog.docker.com/) or ğŸ‘‰ [OpenRouter](https://openrouter.ai/).

---

### 5ï¸âƒ£ Run the App

```bash
docker compose up --build
```

Then open in your browser:
ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

âœ… Your AI chat app is ready to use!

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ app.py                # Streamlit chat app with LangChain
â”œâ”€â”€ Dockerfile            # Container for running the app
â”œâ”€â”€ docker-compose.yaml   # Defines app + model services
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ .env                  # Environment variables (you create this)
```

---

## ğŸ§© How It Works

* The **llm service** (from `docker-compose.yaml`) runs a local LLM using Docker Model Runner (e.g., Gemma).
* The **ai-app service** is a Streamlit app that:

  * Stores chat history in session state
  * Passes the **entire conversation context** to the LLM
  * Lets you **switch between local and remote models** with one checkbox

---

## ğŸš€ Customization

### ğŸ”¹ Change Local Model

Update in `.env`:

```env
LOCAL_MODEL_NAME=ai/gemma3
```

### ğŸ”¹ Change Remote Model

Update in `.env`:

```env
REMOTE_MODEL_NAME=qwen/qwen3-30b-a3b
OPENROUTER_API_KEY=your-key
```

### ğŸ”¹ Add Python Dependencies

Add to `requirements.txt` and rebuild:

```bash
docker compose build
```

---

## ğŸ§¹ Cleanup

When done, free up space:

```bash
docker compose down
docker system prune -a
docker volume prune
```

Check usage:

```bash
docker system df
```

---

## ğŸ“š Helpful Links

* ğŸ³ [Docker Model Runner Docs](https://docs.docker.com/models/)
* ğŸ” [Find Models in Docker Catalog](https://catalog.docker.com/)
* ğŸŒ [OpenRouter](https://openrouter.ai/)
* ğŸ [Streamlit](https://streamlit.io/)
* ğŸ¦œ [LangChain](https://python.langchain.com/)

---

âœ¨ With this setup, you now have your **own AI assistant** that runs locally or on the cloud â€” fully containerized and production-ready ğŸš€

---

Sanju, do you want me to **add a â€œDemo Commandsâ€ section** (with example `docker model pull ai/gemma3` + a sample assistant Q/A output) so it looks even stronger in your GitHub portfolio?
